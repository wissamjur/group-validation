{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# change dir for custom imports\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_name = 'ml-latest-small'\n",
    "\n",
    "testset = pd.read_csv('output/' + dataset_name + '/test.csv')\n",
    "all_predictions = pd.read_csv('output/' + dataset_name + '/all_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "\n",
    "k = 10\n",
    "# eval_map = map_at_k(testset, all_predictions, col_user='userId', col_item='movieId', col_prediction='prediction', k=k)\n",
    "# eval_ndcg = ndcg_at_k(testset, all_predictions, col_user='userId', col_item='movieId', col_prediction='prediction', k=k)\n",
    "eval_precision = precision_at_k(testset, all_predictions, col_user='userId', col_item='movieId', col_prediction='prediction', k=k)\n",
    "# eval_recall = recall_at_k(testset, all_predictions, col_user='userId', col_item='movieId', col_prediction='prediction', k=k)\n",
    "\n",
    "# print(\n",
    "#     \"MAP:\\t%f\" % eval_map,\n",
    "#     \"NDCG:\\t%f\" % eval_ndcg,\n",
    "#     \"Precision@K:\\t%f\" % eval_precision,\n",
    "#     \"Recall@K:\\t%f\" % eval_recall, sep='\\n'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.evaluation.python_evaluation import merge_ranking_true_pred\n",
    "\n",
    "k = 10\n",
    "col_user = \"userId\"\n",
    "col_item = \"movieId\"\n",
    "col_rating = \"rating\"\n",
    "col_prediction = \"prediction\"\n",
    "\n",
    "df_hit, df_hit_count, n_users = merge_ranking_true_pred(\n",
    "    rating_true=testset,\n",
    "    rating_pred=all_predictions,\n",
    "    col_user=col_user,\n",
    "    col_item=col_item,\n",
    "    col_rating=col_rating,\n",
    "    col_prediction=col_prediction,\n",
    "    relevancy_method=\"top_k\",\n",
    "    k=k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate discounted gain for hit items\n",
    "df_dcg = df_hit.copy()\n",
    "# relevance in this case is always 1\n",
    "df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
    "# sum up discount gained to get discount cumulative gain\n",
    "df_dcg = df_dcg.groupby(col_user, as_index=False, sort=False).agg({\"dcg\": \"sum\"})\n",
    "# calculate ideal discounted cumulative gain\n",
    "df_ndcg = pd.merge(df_dcg, df_hit_count, on=[col_user])\n",
    "df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
    "    lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nDCG and precision equations\n",
    "ndcg = (df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users\n",
    "precision = (df_hit_count[\"hit\"] / k).sum() / n_users\n",
    "recall = (df_hit_count[\"hit\"] / df_hit_count[\"actual\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Metric - nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = pd.read_csv('output/' + dataset_name + '/clusters.csv')\n",
    "\n",
    "# group clusters into another dataframe with different representation\n",
    "grouped_clusters = clustered_df.groupby('cluster')['userId'].apply(list).reset_index(name='users_list')\n",
    "grouped_clusters['users_per_cluster'] = grouped_clusters.apply(lambda x: list(set(x.users_list)), axis=1)\n",
    "grouped_clusters = grouped_clusters[['cluster', 'users_per_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric = {}\n",
    "all_clusters_list = grouped_clusters.users_per_cluster.to_list()\n",
    "all_users = len(set(all_predictions.userId.to_list()))\n",
    "\n",
    "for cluster_id, cluster in enumerate(all_clusters_list):\n",
    "    # users in the cluster vs. users in the equiv group\n",
    "    n_cluster_users = len(cluster)\n",
    "    n_cluster_users_equiv = all_users - n_cluster_users\n",
    "\n",
    "    df_ndcg_cluster = df_ndcg.loc[df_ndcg[\"userId\"].isin(cluster)]\n",
    "    df_ndcg_cluster_equiv = df_ndcg.loc[~df_ndcg['userId'].isin(cluster)]\n",
    "\n",
    "    # group metrics\n",
    "    cluster_ndcg = (df_ndcg_cluster[\"dcg\"] / df_ndcg_cluster[\"idcg\"]).sum() / n_cluster_users\n",
    "    cluster_precision = (df_ndcg_cluster[\"hit\"] / k).sum() / n_cluster_users\n",
    "    cluster_recall = (df_ndcg_cluster[\"hit\"] / df_ndcg_cluster[\"actual\"]).sum() / n_cluster_users\n",
    "\n",
    "    # group equiv. metrics\n",
    "    cluster_ndcg_equiv = (df_ndcg_cluster_equiv[\"dcg\"] / df_ndcg_cluster_equiv[\"idcg\"]).sum() / n_cluster_users_equiv\n",
    "    cluster_precision_equiv = (df_ndcg_cluster_equiv[\"hit\"] / k).sum() / n_cluster_users_equiv\n",
    "    cluster_recall_equiv = (df_ndcg_cluster_equiv[\"hit\"] / df_ndcg_cluster_equiv[\"actual\"]).sum() / n_cluster_users_equiv\n",
    "\n",
    "    group_metric[cluster_id] = [\n",
    "        cluster_ndcg,\n",
    "        cluster_ndcg_equiv,\n",
    "        cluster_precision,\n",
    "        cluster_precision_equiv,\n",
    "        cluster_recall,\n",
    "        cluster_recall_equiv\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_metric_df = pd.DataFrame.from_dict(group_metric, orient='index')\\\n",
    "    .reset_index()\\\n",
    "    .rename({\n",
    "        'index': 'cluster',\n",
    "        0: 'cluster-nDCG',\n",
    "        1: 'cluster-nDCG-eq',\n",
    "        2: 'cluster-precision',\n",
    "        3: 'cluster-precision-eq',\n",
    "        4: 'cluster-recall',\n",
    "        5: 'cluster-recall-eq'\n",
    "        }, axis=1)\n",
    "group_metric_df['ndcg'] = ndcg\n",
    "group_metric_df['precision'] = precision\n",
    "group_metric_df['recall'] = recall\n",
    "\n",
    "# save results in csv\n",
    "group_metric_df.to_csv('validation.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52445fb09f682e9366639e37a6414f541c6481f599d97ed36cba6ef55ea50917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
