{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FM model is only supported on Linux.\n",
      "Windows executable can be found at http://www.libfm.org.\n",
      "System version: 3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Cornac version: 1.14.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cornac\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
    "from recommenders.models.cornac.cornac_utils import predict_ranking\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "\n",
    "# change dir for custom imports\n",
    "os.chdir('../')\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Cornac version: {}\".format(cornac.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'ml-25m'\n",
    "fields = ['userId', 'movieId', 'rating']\n",
    "data = pd.read_csv('datasets/' + DATASET_NAME + '/shrunk/ratings_small_v2.csv', usecols=fields)\n",
    "\n",
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Model parameters\n",
    "NUM_FACTORS = 200\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_random_split(data, 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there's synthetic data, add it to the test set\n",
    "# the idea is that the new synthetic data will have different indexes, so we look for those new indexes that are not found in the initial dataset\n",
    "midified_ratings = pd.read_csv('datasets/' + DATASET_NAME + '/modified/ratings_random_experiment.csv', usecols=fields)\n",
    "synthetic_data = midified_ratings.loc[~midified_ratings.index.isin(data.index.to_list())]\n",
    "\n",
    "# concat the synthetic data with the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "502732\n"
     ]
    }
   ],
   "source": [
    "print(len(synthetic_data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 3500\n",
      "Number of items: 15006\n"
     ]
    }
   ],
   "source": [
    "train_set = cornac.data.Dataset.from_uir(train.itertuples(index=False), seed=SEED)\n",
    "\n",
    "print('Number of users: {}'.format(train_set.num_users))\n",
    "print('Number of items: {}'.format(train_set.num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr = cornac.models.BPR(\n",
    "    k=NUM_FACTORS,\n",
    "    max_iter=NUM_EPOCHS,\n",
    "    learning_rate=0.01,\n",
    "    lambda_reg=0.001,\n",
    "    verbose=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016953229904174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 100,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4630fbf0763249c5b214bc80eac7c410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished!\n",
      "Took 31.6342 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    bpr.fit(train_set)\n",
    "print(\"Took {} seconds for training.\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    all_predictions = predict_ranking(bpr, train, usercol='userId', itemcol='movieId', remove_seen=True)\n",
    "print(\"Took {} seconds for prediction.\".format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the prediction and true data frames have the same set of users\n",
    "common_users = set(test['userId']).intersection(set(all_predictions['userId']))\n",
    "rating_true_common = test[test['userId'].isin(common_users)]\n",
    "rating_pred_common = all_predictions[all_predictions['userId'].isin(common_users)]\n",
    "n_users = len(common_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.evaluation.python_evaluation import get_top_k_items\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "df_hit = get_top_k_items(\n",
    "    dataframe=rating_pred_common,\n",
    "    col_user='userId',\n",
    "    col_rating='prediction',\n",
    "    k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../output/' + DATASET_NAME + '/train.csv', index=False)\n",
    "test.to_csv('../output/' + DATASET_NAME + '/test.csv', index=False)\n",
    "df_hit.to_csv('../output/' + DATASET_NAME + '/rankings.csv', index=False)\n",
    "# data.to_csv('./datasets/ml-25m/ratings_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "# eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=k)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_user='userId', col_item='movieId', col_prediction='prediction', k=k)\n",
    "# eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=k)\n",
    "# eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=k)\n",
    "\n",
    "# print(\"MAP:\\t%f\" % eval_map,\n",
    "#       \"NDCG:\\t%f\" % eval_ndcg,\n",
    "#       \"Precision@K:\\t%f\" % eval_precision,\n",
    "#       \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rating_pred_common: all predictions for every user for items they haven't seen\n",
    "# df_hit: top-k recommended items for every user\n",
    "df_hit = get_top_k_items(\n",
    "    dataframe=rating_pred_common,\n",
    "    col_user='userId',\n",
    "    col_rating='prediction',\n",
    "    k=top_k,\n",
    ")\n",
    "# df_hit: all the item hits, the items that the recommender got in the top-k which are in the test set\n",
    "df_hit = pd.merge(df_hit, rating_true_common, on=['userId', 'movieId'])[\n",
    "    ['userId', 'movieId', \"rank\"]\n",
    "]\n",
    "\n",
    "# count the number of hits vs actual relevant items per user\n",
    "df_hit_count = pd.merge(\n",
    "    df_hit.groupby('userId', as_index=False)['userId'].agg({\"hit\": \"count\"}),\n",
    "    rating_true_common.groupby('userId', as_index=False)['userId'].agg(\n",
    "        {\"actual\": \"count\"}\n",
    "    ),\n",
    "    on='userId',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate discounted gain for hit items\n",
    "df_dcg = df_hit.copy()\n",
    "# relevance in this case is always 1\n",
    "df_dcg[\"dcg\"] = 1 / np.log1p(df_dcg[\"rank\"])\n",
    "# sum up discount gained to get discount cumulative gain\n",
    "df_dcg = df_dcg.groupby('userId', as_index=False, sort=False).agg({\"dcg\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "n_users = 610\n",
    "\n",
    "# calculate ideal discounted cumulative gain\n",
    "df_ndcg = pd.merge(df_dcg, df_hit_count, on=['userId'])\n",
    "df_ndcg[\"idcg\"] = df_ndcg[\"actual\"].apply(\n",
    "    lambda x: sum(1 / np.log1p(range(1, min(x, k) + 1)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCG over IDCG is the normalized DCG\n",
    "(df_ndcg[\"dcg\"] / df_ndcg[\"idcg\"]).sum() / n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_true_common"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('group-validation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52445fb09f682e9366639e37a6414f541c6481f599d97ed36cba6ef55ea50917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
